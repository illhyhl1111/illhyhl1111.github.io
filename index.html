<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hyundo Lee</title>

  <meta name="author" content="Hyundo Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2800/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q0957FMC5W"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-Q0957FMC5W');
  </script>
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <!--             <td style="padding:2.5%;width:90%;vertical-align:middle"> -->
                  <p style="text-align:center">
                    <name>Hyundo Lee</name>
                  </p>
                  <p>
                    I am currently a MS/PhD student at <a href="https://www.snu.ac.kr/en/">Seoul National University</a>, 
                    advised by <a href="https://bi.snu.ac.kr">Byoung-Tak Zhang</a> . 
                    I earned my Bachelor's degree in Department of Computer Science and Engineering from Seoul National University.
                    I'm interested in self-supervised representation learning and generative models in the vision domain, and applications to robotics.
                  </p>
                  <p style="text-align:center">
                    <!-- <a href="https://www.dropbox.com/scl/fi/b4jpmny8qbskd5sfv44kt/CV_inwoohwang-20240819.pdf?rlkey=vesu1plllv5w0q2dt4lhgflpd&st=hqrnw5g3&dl=0">CV</a> &nbsp/&nbsp -->
                    <a href="mailto:hdlee@bi.snu.ac.kr">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=tqH-cckAAAAJ&hl=ko&oi=ao">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/illhyhl1111">Github</a>
                    <!-- <a href="https://twitter.com/InwooRyanHwang">Twitter</a> -->
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href=""><img style="width:100%;max-width:100%" alt="profile photo" src="images/hyundo.jpg"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My research is centered on building trustworthy AI systems whose decision making is robust and interpretable, encompassing the fields of representation learning, reinforcement learning, and causal inference. 
                    In particular, my recent works involve developing robust and efficient algorithms for causal inference and causal discovery, with their application for building reliable machine learning models. 
                    Additionally, I am interested in discovering and utilizing useful inductive biases to better align model decisions with human reasoning.
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Education</heading>
                  <ul>
                    <li>(2019.03 - current) MS/Ph.D in Computer Science and Engineering, Seoul National University</li>
                    <li>(2014.03 - 2019.02) BS in Computer Science and Engineering (Minored in Physics and Astronomy), <br>
                         Seoul National University</li>
                    <li>(2012.02 - 2014.02) Highschool, Daegu Il Science High School</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <!-- 		<br> -->
                  <!-- <p> (* equal contribution, <sup>&dagger;</sup> equal advising) </p> -->
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src="images/ILDM.png" alt="ILDM" width="280" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29040">
                    <papertitle>Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models/papertitle>
                  </a>
                  <br>
                  <strong>Hyundo Lee</strong>, Suhyung Choi, Inwoo Hwang‚Ä†, Byoung-Tak Zhang‚Ä†
                  <li> (<strong>Oral</strong>) <em>Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2026 </li>
                  <p> To mitigate spatial inconsistencies in image generation models, we learn to co-generate image and intrinsic scene properties which explicitly represents rich information about the underlying scene, leading to more consistent and natural image generation. </p>
                  <p>
                    <a href="https://arxiv.org/abs/2508.10382">[PDF]</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src="images/DUEL.png" alt="DUEL" width="280" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29040">
                    <papertitle>DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning</papertitle>
                  </a>
                  <br>
                  Won-Seok Choi, <strong>Hyundo Lee</strong>,  Dong-Sig Han, Junseok Park, Heeyeon Koo, Byoung-Tak Zhang
                  <li> <em>Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2024 </li>
                  <p>We propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information.</p>
                  <p>
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29040/29969">[PDF]</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/LBS.png" alt="LBS" width="280" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Learning_Geometry-Aware_Representations_by_Sketching_CVPR_2023_paper.html">
                    <papertitle>Learning Geometry-aware Representations by Sketching</papertitle>
                  </a>
                  <br>
                  <strong>Hyundo Lee</strong>,
                  Inwoo Hwang,
                  Hyunsung Go,
                  Won-Seok Choi,
                  Kibeom Kim,
                  Byoung-Tak Zhang
                  <li> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>,
                    2023 </li>
                  <!-- <br> -->
                  <p>Inspired by human behavior that depicts an image by sketching, we propose a novel representation learning framework
                    that captures geometric information of the scene, such as distance or shape.</p>
                  <p>
                    <a href="https://arxiv.org/abs/2304.08204">[PDF]</a>
                    <a href="https://github.com/illhyhl1111/LearningBySketching">[Code]</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/MDIRL.png" alt="MDIRL" width="280" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1f7b1ed763e9c75e4db74b49b76db5f-Abstract-Conference.html">
                    <papertitle>Robust Imitation via Mirror Descent Inverse Reinforcement Learning</papertitle>
                  </a>
                  <br>
                  Dong-Sig Han, Hyunseo Kim, <strong>Hyundo Lee</strong>, JeHwan Ryu, Byoung-Tak Zhang
                  <li> <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022 </li>
                  <!-- 	      <li> <em>ICML Workshop on Spurious Correlations, Invariance, and Stability</em>, 2023 </li> -->
                  <p>Inspired by a first-order optimization method called mirror descent, this paper proposes to predict a sequence of reward functions, which are iterative solutions for a constrained convex problem.</p>
                  <p>
                    <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1f7b1ed763e9c75e4db74b49b76db5f-Paper-Conference.pdf">[PDF]</a>
                    <a href="https://github.com/dshan4585/mdirl">[Code]</a>
                    <!-- <a href="https://aair-lab.github.io/genplan23/">[Workshop]</a> -->
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/MPART.png" alt="MPART" width="280" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://proceedings.mlr.press/v139/kim21e">
                    <papertitle>Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning
                    </papertitle>
                  </a>
                  <br>
                  Taehyeong Kim, Injune Hwang, <strong>Hyundo Lee</strong>, Hyunseo Kim, Won-Seok Choi, Joseph J Lim, Byoung-Tak Zhang
                  <li> <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2021 </li>
                  <p>We propose Message Passing Adaptive Resonance Theory (MPART) that learns the distribution and topology of input data online. Through message passing on the topological graph, MPART actively queries informative and representative samples, and continuously improves the classification performance using both labeled and unlabeled data.</p>
                  <p>
                    <a href="https://proceedings.mlr.press/v139/kim21e/kim21e.pdf">[PDF]</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/robocup.png" alt="robocup" width="280" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9144932">
                    <papertitle>Visual Perception Framework for an Intelligent Mobile Robot</papertitle>
                  </a>
                  <br>
                  Chung-Yeon Lee, <strong>Hyundo Lee</strong>, Injune Hwang, Byoung-Tak Zhang
                  <li> <em>17th International Conference on Ubiquitous Robots (<strong>UR</strong>)</em>, 2020 </li>
                  <p>We present a visual perception framework for an intelligent mobile robot. Based on the robot operating system middleware, our framework integrates a broad set of advanced algorithms capable of recognising people, objects and human poses, as well as describing observed scenes.</p>
                  <p>
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9144932">[PDF]</a>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/orbslam.png" alt="orbslam" height="200" style="margin-left: auto; margin-right: auto; display: block; border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content_ICCVW_2019/html/DL4VSLAM/Lee_Spatial_Perception_by_Object-Aware_Visual_Scene_Representation_ICCVW_2019_paper.html">
                    <papertitle>Spatial Perception by Object-Aware Visual Scene Representation</papertitle>
                  </a>
                  <br>
                  Chung-Yeon Lee, <strong>Hyundo Lee</strong>, Injune Hwang, Byoung-Tak Zhang
                  <li> <em>ICCV Workshop on Deep Learning for Visual SLAM, 2019 </li>
                  <p>We present a spatial perception framework that uses an object-aware visual scene representation to enhance the spatial abilities. The proposed representation compensates for aberrations of conventional geometric scene representations by fusing those representations with semantic features extracted from perceived objects.</p>
                  </p>
                  <p>
                    <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/DL4VSLAM/Lee_Spatial_Perception_by_Object-Aware_Visual_Scene_Representation_ICCVW_2019_paper.pdf">[PDF]</a>
                    <!-- <a href="https://sites.google.com/view/causal-sequential-decisions/home">[Workshop]</a> -->
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/EncGAN.png" alt="EncGAN" width="280" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/1906.00541">
                    <papertitle>Encoder-Powered Generative Adversarial Networks</papertitle>
                  </a>
                  <br>
                  Jiseob Kim, Seungjae Jung, <strong>Hyundo Lee</strong>, Byoung-Tak Zhang
                  <li> <em>arXiv</em>, 2019 </li>
                  <p> We present an encoder-powered generative adversarial network (EncGAN) that is able to learn both the multi-manifold structure and the abstract features of data.</p> 
                  <p>
                    <a href="https://arxiv.org/abs/1906.00541">[PDF]</a>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Work Experience</heading>
                  <ul>
                    <li>(Sep 2021 - May 2022) External collaborator, Naver AI (host: <a href="http://wityworks.com">Jin-Hwa Kim</a>)</li>
                    <li>(Aug 2012 - May 2014) Mandatory Military service, Korean Augmentation To the US Army (KATUSA)</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Academic Services</heading>
                  <ul>
                    <li>Conference Reviewer: NeurIPS (2023-2024), ICLR (2024), ICML (2024), AAAI (2025), AISTATS (2024), CLeaR (2024),
                      CVPR (2023-2024), ICCV (2023), ECCV (2024), ICRA (2024)</li>
                    <li> Journal Reviewer: IEEE Trans. Multimedia </li>
                    <li>Workshop Reviewer</li>
                    <ul>
                      <li>Workshop on Spurious Correlations, Invariance, and Stability (ICML 2023)</li>
                      <li>Workshop on Causal Representation Learning (NeurIPS 2023)</li>
                      <li>Workshop on Reinforcement Learning Beyond Rewards (RLC 2024)</li>
                    </ul>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Invited Talks</heading>
                  <ul>
                    <li>[Jun 2024] IITP Workshop</li>
                    <li>[Sep 2023] IITP Workshop</li>
                    <li>[May 2023] SNU AIIS Retreat</li>
                    <li>[Dec 2022] Korea Software Congress</li>
                    <li>[Nov 2022] Kakao Enterprise TechTalk </li>
                    <li>[Nov 2022] SNU AIIS Retreat</li>
                    <li>[Oct 2022] NAVER TechTalk</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Honors and Awards</heading>
                  <ul>
                    <li>Youlchon AI Star Scholarship, 2024</li>
                    <li>UAI Scholarship, 2024</li>
                    <li>NAVER PhD Fellowship, 2022</li>
                    <li>NeurIPS Scholarship, 2022</li>
                    <li>BK21 Plus Scholarship, Republic of Korea</li>
                    <li>National Science and Technology Scholarship, Korea Student Aid Foundation </li>
                    <li>Gold Award, The Korean Mathematical Olympiad (KMO)</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Teaching Experience</heading>
                  <ul>
                    <li>[CS204] Discrete Mathematics, KAIST, 2016S - 2017F</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table> -->


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    The source of this website is from <a href="https://jonbarron.info/">here</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
